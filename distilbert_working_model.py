# -*- coding: utf-8 -*-
"""DistilBERT_working_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NV4RwbeWknN_HqQh_hE4H-OcPrGtiPii

# **DistilBERT working model**

Model code adapted from:

https://swatimeena989.medium.com/bert-text-classification-using-keras-903671e0207d
"""

!pip install transformers

# Import libraries
import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import re
import string
import unicodedata
import math
import nltk
from nltk.corpus import stopwords
import keras
from tqdm import tqdm
import pickle
from keras.models import Model
import keras.backend as K
from sklearn.metrics import confusion_matrix,f1_score,classification_report
import matplotlib.pyplot as plt
from matplotlib import pyplot
from keras.callbacks import ModelCheckpoint
import itertools
from keras.models import load_model
from sklearn.utils import shuffle
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, TFDistilBertModel, DistilBertConfig

# Preprocessing functions

def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

def clean_stopwords_shortwords(w):
    stopwords_list=stopwords.words('english')
    words = w.split() 
    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 2]
    return " ".join(clean_words) 

def preprocess_sentence(w):
    w = unicode_to_ascii(w.lower().strip())
    w = re.sub(r"([?.!,¿])", r" ", w)
    w = re.sub(r'[" "]+', " ", w)
    w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)
    w=clean_stopwords_shortwords(w) 
    w=re.sub(r'@\w+', '',w)
    return w

# Load Bitcoin Twitter dataset to fine-tune model
df = pd.read_csv('BTC_BERT_Train_Dataset_Balanced_Large_B.csv')
sentiments = df['Sentiment'].values.tolist()
tweets = df['Clean_Tweet'].values.tolist()
df.head()

df.shape

# Implement stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Clean the text column using preprocess_sentence function defined above
df['Clean_Tweet'] = df['Clean_Tweet'].map(preprocess_sentence) 
df.head()

# Load the DistilBERT tokenizer and suitable DistilBERT model
num_classes=len(df.Sentiment.unique())
DistilBERT_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
DistilBERT_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',
                                                                         num_labels=num_classes)

sentences = df['Clean_Tweet']
labels = df['Sentiment']
len(sentences), len(labels)

# Load the sentences into the DistilBERT Tokenizer
input_ids = []
attention_masks = []

for sent in sentences:
    DistilBERT_inp = DistilBERT_tokenizer.encode_plus(sent,
        add_special_tokens = True,
        max_length =64, 
        pad_to_max_length = True,
        return_attention_mask = True)
    input_ids.append(DistilBERT_inp['input_ids'])
    attention_masks.append(DistilBERT_inp['attention_mask'])

input_ids = np.asarray(input_ids)
attention_masks = np.array(attention_masks)
labels = np.array(labels)

len(input_ids), len(attention_masks), len(labels)

# Save and load the data into pickle files

# The dump() function serializes the data and writes it to the file
with open('pickled_data_file_input.pkl', 'wb') as fid1:
     pickle.dump(input_ids, fid1)

# Read the data from the file
with open('pickled_data_file_input.pkl', 'rb') as fid1:
     input_ids = pickle.load(fid1)

# The dump() function serializes the data and writes it to the file
with open('pickled_data_file_attention.pkl', 'wb') as fid2:
     pickle.dump(attention_masks, fid2)

# Read the data from the file
with open('pickled_data_file_attention.pkl', 'rb') as fid2:
     attention_masks = pickle.load(fid2)

# The dump() function serializes the data and writes it to the file
with open('pickled_data_file_labels.pkl', 'wb') as fid3:
     pickle.dump(labels, fid3)

# Read the data from the file
with open('pickled_data_file_labels.pkl', 'rb') as fid3:
     labels = pickle.load(fid3)

print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))

# Split data into train and validation datasets
train_inp, val_inp, train_label, val_label, train_mask, val_mask = train_test_split(input_ids, labels, attention_masks, test_size = 0.2)
print('Train inp shape {} Val input shape {}\nTrain label shape {} Val label shape {}\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))

# Set the loss, metric and optimizer
log_dir='logs'

model_save_path='DistilBERT_model.h5'

callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,
                                                save_weights_only=True,
                                                monitor='val_loss',
                                                mode='min',
                                                save_best_only=True),
                          keras.callbacks.TensorBoard(log_dir=log_dir)]

print('\nDistilBERT Model', DistilBERT_model.summary())

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)

DistilBERT_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])

tf.config.run_functions_eagerly(True)

# Train the model
history = DistilBERT_model.fit([train_inp,train_mask],
                            train_label,batch_size=256,
                            epochs=2,
                            validation_data=([val_inp,val_mask],val_label),
                            callbacks=callbacks)

"""## **Evaluate model performance**"""

history.history

# Evaluate model loss and accuracy
loss, acc = DistilBERT_model.evaluate([val_inp,val_mask],val_label)
print("Model val_loss: {:5.2f}%".format(100 * loss), "Model val_accuracy: {:5.2f}%".format(100 * acc))

# plot accuracy
pyplot.plot(history.history['accuracy'], label='training')
pyplot.plot(history.history['val_accuracy'], label='validation')
pyplot.legend()
pyplot.title("Training & Validation Accuracy")
pyplot.show()

# plot loss
pyplot.plot(history.history['loss'], label='training')
pyplot.plot(history.history['val_loss'], label='validation')
pyplot.legend()
pyplot.title("Training & Validation Loss")
pyplot.show()

'''Confusion matrix code adapted from:
https://stackoverflow.com/questions/68245499/plot-confusion-matrix-from-roberta-model'''

trained_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',
                                                                         num_labels=num_classes)
trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])
trained_model.load_weights(model_save_path)

preds = trained_model.predict([val_inp,val_mask],batch_size=32)
pred_labels = np.argmax(preds.logits, axis=1)
conf_matrix = confusion_matrix(val_label, pred_labels)
print('conf_matrix ', conf_matrix)
    
fig, ax = plt.subplots(figsize=(7.5, 7.5))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

# Evaluate model
tn, fp, fn, tp = confusion_matrix(val_label, pred_labels).ravel()
Acc = (tp+tn)/(tp+tn+fp+fn)
Prec = tp/(tp+fp)
TPR = tp/(tp+fn)
FPR = fp/(tn+fp)
F1 = tp/(tp+((fn+fp)/2))
TNR = tn/(tn+fp)
FNR = fn/(fn+tp)
print("Performance over the validation dataset")
print("Accuracy: {:.4f}, Precision: {:.4f}, True Positive Rate (Recall): {:.4f}, False Positive Rate: {:.4f}".format(Acc,Prec,TPR,FPR))
print("F1 Score: {:.4f}, True Negative Rate: {:.4f}, False Negative Rate: {:.4f}".format(F1,TNR,FNR))

"""## **Predict sentiment**"""

test_sentence = "#bitcoin is great"

predict_input = DistilBERT_tokenizer.encode(test_sentence,
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")

tf_output = DistilBERT_model.predict(predict_input)[0]

tf_prediction = tf.nn.softmax(tf_output, axis=1)
labels = ['Negative','Positive']
label = tf.argmax(tf_prediction, axis=1)
label = label.numpy()
print(labels[label[0]])

"""## **Load & preprocess new uncategorised Bitcoin Twitter dataset**"""

# Load Twitter tweets
lines = []
with open('BTCtweets_28Feb2021_EXP.txt') as f:
    lines = f.readlines()

# Check no. of tweets in dataset
print(len(lines))

# Separate tweet text from date on each line
new_lines = []
for i in range(len(lines)):
  newline = lines[i].split('""')
  new_lines.append(newline)

print(len(new_lines))

# Convert list into pandas dataframe
df_fresh = pd.DataFrame(new_lines, columns=['Clean_Tweet', 'Date'])
df_fresh.head()

# Drop first row
df_fresh = df_fresh.iloc[1:]
df_fresh.head()

# Clean the tweets
def cleanTwt(twt):
    twt = re.sub("#bitcoin", 'bitcoin', twt) # remove the '#' from bitcoin
    twt = re.sub("#Bitcoin", 'Bitcoin', twt) # remove the '#' from Bitcoin
    twt = re.sub('#[A-Za-z0-9]+', '', twt) # remove any string with a '#'
    twt = re.sub('\\n', '', twt) # remove the '\n' string
    twt = re.sub('https:\/\/\S+', '', twt) # remove any hyperlinks

    twt = re.sub('(RT|via)((?:\\b\\W*@\\w+)+)', ' ', twt)
    twt = re.sub(r'@\S+', '', twt)
    twt = re.sub('&amp', ' ', twt)
    twt = re.sub(r"\s{2,}", " ", twt)  # remove multiple whitespaces
    twt = re.sub(r"\s+t\s+", "'t ", twt)  # replace separately standing "t" as 't
    twt = re.sub(r'"', "", twt)
    twt = re.sub(r"http stks co \w+\s*", "", twt)
    twt = re.sub(r"\w+tag_", "", twt)
    twt = re.sub(r"\w+tag\s*", "", twt)
    twt = re.sub(r":", "", twt)
    # remove punctuation and whitespaces from both ends
    translator = str.maketrans('', '', string.punctuation)
    twt = twt.translate(translator).strip()
  
    return twt

df_fresh['Clean_Tweet'] = df_fresh['Tweet'].apply(cleanTwt)
df_fresh.head()

# Select specified no. of tweets at random for smaller dataset
df_fresh = df_fresh.sample(n=20000)
df_fresh.shape

df_fresh.shape

"""## **Predict sentiment on new Twitter dataset**"""

# Create function for model to predict sentiment for each tweet

def ModelPredictsSentiment(twt):
    sentence = twt['Clean_Tweet']

    predict_input = DistilBERT_tokenizer.encode(sentence,
                                 truncation=True,
                                 padding=True,
                                 return_tensors="tf")

    tf_output = DistilBERT_model.predict(predict_input)[0]

    tf_prediction = tf.nn.softmax(tf_output, axis=1)
    labels = ['Negative','Positive']
    label = tf.argmax(tf_prediction, axis=1)
    label = label.numpy()
    return (labels[label[0]])

df_fresh['Model_Sentiment'] = df_fresh.apply(lambda twt: ModelPredictsSentiment(twt), axis=1)

df_fresh.head()

df_fresh['Model_Sentiment'].value_counts()

# Create bar chart to show the count of Positive and Negative sentiments
df_fresh['Model_Sentiment'].value_counts().plot(kind="bar")
plt.title("Sentiment Analysis Bar Graph")
plt.xlabel("Sentiment")
plt.ylabel("No. of tweets")
plt.show()

# Export dataframe to csv file
df_fresh.to_csv('DistilBERT_model_sentiment_data_28Feb.csv')